import torch
import pytorch_lightning as pl
from omegaconf import OmegaConf
import argparse
import sys

sys.path.insert(0, '/root/autodl-tmp/newwork')

from models.first_stage_kl_atom import AutoencoderKL


def load_and_freeze_encoder_decoder_base(model, pretrained_path):
    """åŠ è½½é¢„è®­ç»ƒ encoder + decoderåŸºç¡€éƒ¨åˆ†ï¼Œå¹¶å†»ç»“"""
    print(f"\n{'='*80}")
    print(f"åŠ è½½é¢„è®­ç»ƒæƒé‡: {pretrained_path}")
    print(f"{'='*80}")

    checkpoint = torch.load(pretrained_path, map_location='cpu')
    state_dict = checkpoint['state_dict']

    # æå– encoder + decoderåŸºç¡€éƒ¨åˆ†ï¼ˆæ’é™¤upsamplerï¼‰
    pretrained_state = {}
    for key, value in state_dict.items():
        # åŠ è½½ encoder å…¨éƒ¨
        if key.startswith('encoder.'):
            pretrained_state[key] = value
        # åŠ è½½ decoderï¼Œä½†æ’é™¤ upsampler
        elif key.startswith('decoder.') and not key.startswith('decoder.upsampler'):
            pretrained_state[key] = value

    # åŠ è½½å‚æ•°ï¼ˆupsamplerä¼šmissingï¼Œè¿™æ˜¯é¢„æœŸçš„ï¼‰
    missing_keys, unexpected_keys = model.load_state_dict(pretrained_state, strict=False)

    print(f"\nâœ… åŠ è½½äº† {len(pretrained_state)} ä¸ªé¢„è®­ç»ƒå‚æ•°")
    print(f"   Encoder: {len([k for k in pretrained_state if k.startswith('encoder.')])}")
    print(f"   Decoder (ä¸å«upsampler): {len([k for k in pretrained_state if k.startswith('decoder.')])}")

    # æ£€æŸ¥missingçš„æ˜¯å¦éƒ½æ˜¯upsampleræˆ–lossï¼ˆé¢„æœŸçš„ï¼‰
    missing_upsampler = [k for k in missing_keys if 'upsampler' in k]
    missing_others = [k for k in missing_keys if 'upsampler' not in k and 'loss' not in k and 'quant' not in k]

    print(f"\n   Missing upsampler keys (é¢„æœŸ): {len(missing_upsampler)}")
    if missing_others:
        print(f"   âš ï¸  Missing other keys (ä¸åº”è¯¥æœ‰): {len(missing_others)}")
        for k in missing_others[:5]:
            print(f"      {k}")

    # å†»ç»“ encoder + decoderåŸºç¡€éƒ¨åˆ†
    frozen_params = 0
    trainable_params_count = 0

    for name, param in model.named_parameters():
        # å†»ç»“encoderå’ŒdecoderåŸºç¡€éƒ¨åˆ†ï¼ˆä¸å«upsamplerï¼‰
        if name.startswith('encoder.') or \
           (name.startswith('decoder.') and not name.startswith('decoder.upsampler')):
            param.requires_grad = False
            frozen_params += 1
        else:
            trainable_params_count += 1

    print(f"\nğŸ”’ å†»ç»“äº† {frozen_params} ä¸ªå‚æ•°ï¼ˆEncoder + DecoderåŸºç¡€éƒ¨åˆ†ï¼‰")
    print(f"ğŸ¯ å¯è®­ç»ƒ {trainable_params_count} ä¸ªå‚æ•°ï¼ˆä»… GaussianQuery Upsamplerï¼‰")

    # ç»Ÿè®¡å¯è®­ç»ƒå‚æ•°æ•°é‡
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())

    print(f"\nğŸ“Š å‚æ•°ç»Ÿè®¡:")
    print(f"   å¯è®­ç»ƒ: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)")
    print(f"   æ€»å‚æ•°: {total_params:,}")
    print(f"{'='*80}\n")

    return model


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--config', type=str, required=True, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--pretrained', type=str, required=True, help='é¢„è®­ç»ƒ checkpoint è·¯å¾„')
    parser.add_argument('--gpus', type=str, default='0', help='GPU ç´¢å¼•')
    parser.add_argument('--seed', type=int, default=23, help='éšæœºç§å­')
    args = parser.parse_args()

    # è®¾ç½®éšæœºç§å­
    pl.seed_everything(args.seed)

    # åŠ è½½é…ç½®
    config = OmegaConf.load(args.config)

    # è§£æ GPU
    gpu_list = [int(x) for x in args.gpus.split(',')]
    print(f"ä½¿ç”¨ GPU: {gpu_list}")

    # åˆ›å»ºæ¨¡å‹
    model = AutoencoderKL(**config.model.params)

    # åŠ è½½é¢„è®­ç»ƒ encoder + decoderåŸºç¡€éƒ¨åˆ†ï¼Œå¹¶å†»ç»“
    model = load_and_freeze_encoder_decoder_base(model, args.pretrained)

    # åˆ›å»ºæ•°æ®æ¨¡å—
    from data.datamodule import DataModuleFromConfig
    data = DataModuleFromConfig(**config.data.params)

    # åˆ›å»º Trainer
    trainer_config = OmegaConf.to_container(config.lightning.trainer, resolve=True)
    trainer_config['accelerator'] = 'gpu'
    trainer_config['devices'] = gpu_list

    trainer = pl.Trainer(**trainer_config)

    # å¼€å§‹è®­ç»ƒ
    print(f"\n{'='*80}")
    print("å¼€å§‹è®­ç»ƒï¼ˆä»…è®­ç»ƒ GaussianQuery Upsamplerï¼‰")
    print(f"{'='*80}\n")

    trainer.fit(model, data)


if __name__ == '__main__':
    main()
